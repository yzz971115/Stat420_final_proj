---
title: "Predict Prices for Your Dream House"
author: "STAT 420, Summer 2019, Zhenzhou Yang (zy29), Swan Htun (swanh2), Mike Kramer (mkramer4)"
date: '7/16/2019'
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---

# Introduction

## Overview

In this project, we plan to find a suitable linear model to predict the prices of residential houses locating in Ames, Iowa, based on their attributes provided. This model is meaningful that the customers who want to buy a new house may rely on it to have a rough estimation. 

In this project, many of the topics will be included, some of them will be:

- Multiple linear regression
- Outlier diagnostics
- Model building 
- Model selection

## Dataset Introduction

### Source

The `Ames Housing dataset` we use in this project is provided by Dean De Cock from Truman State University for [`Kaggle competition`](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data).

### Introduction

The origin dataset mainly includes the sale of individual residential houses in Ames, Iowa from 2006 to 2010. 

It contains 2919 observations with 23 nominal, 23 ordinal, 14 discrete and 20 continuous variables, which are directly related to property sales. And we just use half of it which contains 1460 observations and split the data by half into our train and test dataset.

In our project, we choose the variables which we think have significant influence in affecting the price of a house to somplify the data and try to reduce the collinearity between different predictors at beginning:

- **SalePrice**: the property's sale price in dollars. This is our target to predict.
- **LotArea**: Lot size in square feet.
- **MSZoning**: Identifies the general zoning classification of the sale. It has following types:
  - A:	Agriculture
  - C:	Commercial
  - FV:	Floating Village Residential
  - I:	Industrial
  - RH:	Residential High Density
  - RL:	Residential Low Density
  - RP:	Residential Low Density Park 
  - RM:	Residential Medium Density
- **LotShape**: General shape of property. It has the following levels:
  - Reg:	Regular	
  - IR1:	Slightly irregular
  - IR2:	Moderately Irregular
  - IR3:	Irregular
- **OverallQual**: Overall material and finish quality. It is an ordinal categorical variable range from 1 to 10 in origin data, which indicating the quality from very poor to very excellent.
- **YearBuilt**: Original construction date.
- **TotalBsmtSF**: Total square feet of basement area.
- **LowQualFinSF**: Low quality finished square feet (all floors).
- **BedroomAbvGr**: Bedrooms above grade (does NOT include basement bedrooms).
- **FullBath**: Full bathrooms above grade.
- **GarageArea**: Size of garage in square feet.

### Data

In this section, we will take a look at the data which has been modified by us for the project.

- Train data
```{r,message=FALSE}
library(readr)
data_raw = read_csv("house price.csv")

# split the data
set.seed(42)
train_idx = sample(1 : nrow(data_raw), nrow(data_raw) / 2)
test_idx = setdiff(seq(1 : nrow(data_raw)), train_idx)

train_raw = data_raw[train_idx,]
test_raw = data_raw[test_idx,]

# select variables we want to use
train = subset(train_raw, select = c("SalePrice", "LotArea", "MSZoning", "LotShape", "OverallQual", "YearBuilt", "TotalBsmtSF", "LowQualFinSF", "BedroomAbvGr", "FullBath", "GarageArea"))

# show a few lines
head(train, 5)

# let's take a galance at the response variable
head(train$SalePrice, 10)
```

- Test data
```{r,message=FALSE} 
# select variables we want to use
test = subset(test_raw, select = c("SalePrice", "LotArea", "MSZoning", "LotShape", "OverallQual", "YearBuilt", "TotalBsmtSF", "LowQualFinSF", "BedroomAbvGr", "FullBath", "GarageArea"))

# show a few lines
head(test, 5)

# let's take a galance at the response variable
head(test$SalePrice, 10)
```

# Methods
 
## Data Checking

- First of all, we will check the validality of our data and omit the missing data.
```{r}
# train set
sum(is.na(train))

# test set
sum(is.na(test))
```

It seems that our data set is very good! There is no missing data in it.

- The next step is to convert our categorical variables into factor.
```{r}
# train set
train$MSZoning = as.factor(train$MSZoning)
train$LotShape = as.factor(train$LotShape)

# test set
test$MSZoning = as.factor(test$MSZoning)
test$LotShape = as.factor(test$LotShape)
```

And also, since `OverallQual` has 10 levels which maybe too complicated for us to use, we just modify it into three different levels, the original level 1-3 will be **poor**, level 4-7 will be **average**, the rest, level 8-10, will be **excellent**.
```{r}
# train set
train$OverallQual[train$OverallQual == 3 | train$OverallQual == 2 | train$OverallQual == 1] = "poor"
train$OverallQual[train$OverallQual == 4 | train$OverallQual == 5 | train$OverallQual == 6 | train$OverallQual == 7] = "average"
train$OverallQual[train$OverallQual == 8 | train$OverallQual == 9 | train$OverallQual == 10] = "excellent"

train$OverallQual = as.factor(train$OverallQual)

# test set
test$OverallQual[test$OverallQual == 3 | test$OverallQual == 2 | test$OverallQual == 1] = "poor"
test$OverallQual[test$OverallQual == 4 | test$OverallQual == 5 | test$OverallQual == 6 | test$OverallQual == 7] = "average"
test$OverallQual[test$OverallQual == 8 | test$OverallQual == 9 | test$OverallQual == 10] = "excellent"

test$OverallQual = as.factor(test$OverallQual)
```

- Then, we will to check the distribution of our target -- `SalePrice`. 
```{r}
hist(train$SalePrice, col = 'darkorange', main = 'Sale Price', prob = TRUE)
```

It seems that the `SalePrice` is right skewed, so we will try to make some transformations to make it look better.
```{r}
hist(log(train$SalePrice), col = 'darkorange', main = 'Sale Price', prob = TRUE)
qqnorm(log(train$SalePrice))
qqline(log(train$SalePrice))
```

After the log transformation, the distribution looks much better now.

## Predictor Choosing

- We first modify the factor variables into numeric ones to check the correlation between each variables.
```{r}
# convert variables
train$MSZoning = as.numeric(train$MSZoning)
train$LotShape = as.numeric(train$LotShape)
train$OverallQual = as.numeric(train$OverallQual)

# check correlation
round(cor(train), 2)

# convert back 
train$MSZoning = as.factor(train$MSZoning)
train$LotShape = as.factor(train$LotShape)
train$OverallQual = as.factor(train$OverallQual)
```

From the table above, we do not find any significant colliearity between different variables. We can choose all of them together for fitting.

## Model Fitting

- Firstly, we fit the model with all variables as predictors
```{r}
model = lm(SalePrice ~ ., data = train)
summary(model)
```

We now will use `ANOVA` to test the significance of the factor variables.
