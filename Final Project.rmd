---
title: "Predict Prices for Your Dream House"
author: "STAT 420, Summer 2019, Zhenzhou Yang (zy29), Swan Htun (swanh2), Mike Kramer (mkramer4)"
date: '7/16/2019'
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---

# Introduction

## Overview

In this project, we plan to find a suitable linear model to predict the prices of residential houses locating in Ames, Iowa, based on their attributes provided. This model is meaningful that the customers who want to buy a new house may rely on it to have a rough estimation. 

In this project, many of the topics will be included, some of them will be:

- Multiple linear regression
- Outlier diagnostics
- Model building 
- Model selection

## Dataset Introduction

### Source

The `Ames Housing dataset` we use in this project is provided by Dean De Cock from Truman State University for [`Kaggle competition`](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data).

### Introduction

The origin dataset mainly includes the sale of individual residential houses in Ames, Iowa from 2006 to 2010. 

It contains 2919 observations with 23 nominal, 23 ordinal, 14 discrete and 20 continuous variables, which are directly related to property sales. And we just use half of it which contains 1460 observations and split the data by half into our train and test dataset.

In our project, we choose the variables which we think have significant influence in affecting the price of a house to somplify the data and try to reduce the collinearity between different predictors at beginning:

- **SalePrice**: the property's sale price in dollars. This is our target to predict.
- **LotArea**: Lot size in square feet.
- **MSZoning**: Identifies the general zoning classification of the sale. It has following types:
  - A:	Agriculture
  - C:	Commercial
  - FV:	Floating Village Residential
  - I:	Industrial
  - RH:	Residential High Density
  - RL:	Residential Low Density
  - RP:	Residential Low Density Park 
  - RM:	Residential Medium Density
- **LotShape**: General shape of property. It has the following levels:
  - Reg:	Regular	
  - IR1:	Slightly irregular
  - IR2:	Moderately Irregular
  - IR3:	Irregular
- **OverallQual**: Overall material and finish quality. It is an ordinal categorical variable range from 1 to 10 in origin data, which indicating the quality from very poor to very excellent.
- **YearBuilt**: Original construction date.
- **TotalBsmtSF**: Total square feet of basement area.
- **LowQualFinSF**: Low quality finished square feet (all floors).
- **BedroomAbvGr**: Bedrooms above grade (does NOT include basement bedrooms).
- **FullBath**: Full bathrooms above grade.
- **GarageArea**: Size of garage in square feet.

### Data

In this section, we will take a look at the data which has been modified by us for the project.

- Train data
```{r,message=FALSE}
library(readr)
data_raw = read_csv("house price.csv")

# split the data
set.seed(42)
train_idx = sample(1 : nrow(data_raw), nrow(data_raw) / 2)
test_idx = setdiff(seq(1 : nrow(data_raw)), train_idx)

train_raw = data_raw[train_idx,]
test_raw = data_raw[test_idx,]

# select variables we want to use
train = subset(train_raw, select = c("SalePrice", "LotArea", "MSZoning", "LotShape", "OverallQual", "YearBuilt", "TotalBsmtSF", "LowQualFinSF", "BedroomAbvGr", "FullBath", "GarageArea"))

# show a few lines
head(train, 5)

# let's take a galance at the response variable
head(train$SalePrice, 10)
```

- Test data
```{r,message=FALSE} 
# select variables we want to use
test = subset(test_raw, select = c("SalePrice", "LotArea", "MSZoning", "LotShape", "OverallQual", "YearBuilt", "TotalBsmtSF", "LowQualFinSF", "BedroomAbvGr", "FullBath", "GarageArea"))

# show a few lines
head(test, 5)

# let's take a galance at the response variable
head(test$SalePrice, 10)
```

# Methods
 
## Data Checking

- First of all, we will check the validality of our data and omit the missing data.
```{r}
# train set
sum(is.na(train))

# test set
sum(is.na(test))
```

It seems that our data set is very good! There is no missing data in it.

- The next step is to convert our categorical variables into factor.
```{r}
# train set
train$MSZoning = as.factor(train$MSZoning)
train$LotShape = as.factor(train$LotShape)

# test set
test$MSZoning = as.factor(test$MSZoning)
test$LotShape = as.factor(test$LotShape)
```

And also, since `OverallQual` has 10 levels which maybe too complicated for us to use, we just modify it into three different levels, the original level 1-3 will be **poor**, level 4-7 will be **average**, the rest, level 8-10, will be **excellent**.
```{r}
# train set
train$OverallQual[train$OverallQual == 3 | train$OverallQual == 2 | train$OverallQual == 1] = "poor"
train$OverallQual[train$OverallQual == 4 | train$OverallQual == 5 | train$OverallQual == 6 | train$OverallQual == 7] = "average"
train$OverallQual[train$OverallQual == 8 | train$OverallQual == 9 | train$OverallQual == 10] = "excellent"

train$OverallQual = as.factor(train$OverallQual)

# test set
test$OverallQual[test$OverallQual == 3 | test$OverallQual == 2 | test$OverallQual == 1] = "poor"
test$OverallQual[test$OverallQual == 4 | test$OverallQual == 5 | test$OverallQual == 6 | test$OverallQual == 7] = "average"
test$OverallQual[test$OverallQual == 8 | test$OverallQual == 9 | test$OverallQual == 10] = "excellent"

test$OverallQual = as.factor(test$OverallQual)
```

- Then, we will to check the distribution of our target -- `SalePrice`. 
```{r}
hist(train$SalePrice, col = 'darkorange', main = 'Sale Price', prob = TRUE)
```

It seems that the `SalePrice` is right skewed, so we will try to make some transformations to make it look better.
```{r}
hist(log(train$SalePrice), col = 'darkorange', main = 'Sale Price', prob = TRUE)
qqnorm(log(train$SalePrice))
qqline(log(train$SalePrice))
```

After the log transformation, the distribution looks much better now.

## Predictor Choosing

- We first modify the factor variables into numeric ones to check the correlation between each variables.
```{r}
# convert variables
train$MSZoning = as.numeric(train$MSZoning)
train$LotShape = as.numeric(train$LotShape)
train$OverallQual = as.numeric(train$OverallQual)

# check correlation
round(cor(train), 2)

# convert back 
train$MSZoning = as.factor(train$MSZoning)
train$LotShape = as.factor(train$LotShape)
train$OverallQual = as.factor(train$OverallQual)
```

From the table above, we do not find any significant colliearity between different variables. We can choose all of them together for fitting.

## Model Fitting

- Before we create any models, we prefer to define some functions we may use later first.
```{r,message=FALSE}
library(lmtest)

# Fitted versus Residuals Plot
plot_fitted_resid = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
  plot(fitted(model), resid(model), 
       col = pointcol, pch = 20, cex = 1.5,
       xlab = "Fitted", ylab = "Residuals")
  abline(h = 0, col = linecol, lwd = 2)
}

# Q-Q plot
plot_qq = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
  qqnorm(resid(model), col = pointcol, pch = 20, cex = 1.5)
  qqline(resid(model), col = linecol, lwd = 2)
}

# bptest
get_bp_decision = function(model, alpha = 0.05) {
  decide = unname(bptest(model)$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

# shapiro wilk test
get_sw_decision = function(model, alpha = 0.05) {
  decide = unname(shapiro.test(resid(model))$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

# LOOCV_RMSE
get_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

# ajusted R2
get_adj_r2 = function(model) {
  summary(model)$adj.r.squared
}
```

### Candidate Models

- Firstly, we will start with an intersection model with all possible two-way interaction between predictors and an additive model with all possible predictors.
```{r}
model_add = lm(SalePrice ~ ., data = train)
model_inter = lm(SalePrice ~ . ^ 2, data = train)
```

- We will also try to find some models with transformed variablesã€‚

As we analyzed before, after the log transformation, the SalePrice will be more normalized.
```{r}
log_add = lm(log(SalePrice) ~ ., data = train)
log_inter = lm(log(SalePrice) ~ . ^ 2, data = train)
```

Then, we will check the realtionship between `SalePrice` and each numerical predictors
```{r}
par(mfrow = c(3, 2))
plot(SalePrice ~ LotArea, data = train, col = "darkgrey")
plot(SalePrice ~ YearBuilt, data = train, col = "darkgrey")
plot(SalePrice ~ TotalBsmtSF, data = train, col = "darkgrey")
plot(SalePrice ~ LowQualFinSF, data = train, col = "darkgrey")
plot(SalePrice ~ BedroomAbvGr, data = train, col = "darkgrey")
```

We try to use some transformations to make the plot looks like more linearly.
```{r}
par(mfrow = c(3, 2))
plot(log(SalePrice) ~ log(LotArea), data = train, col = "darkgrey")
plot(log(SalePrice) ~ YearBuilt, data = train, col = "darkgrey")
plot(log(SalePrice) ~ TotalBsmtSF, data = train, col = "darkgrey")
plot(SalePrice ~ I(TotalBsmtSF ^ 2), data = train, col = "darkgrey")
plot(log(SalePrice) ~ LowQualFinSF, data = train, col = "darkgrey")
plot(log(SalePrice) ~ log(BedroomAbvGr), data = train, col = "darkgrey")
```

Based on the above plots, we add several new models.
```{r}
trans_mod_1 = lm(log(SalePrice) ~ . - LotArea + log(LotArea), data = train)
trans_mod_1_inter = lm(log(SalePrice) ~ (. - LotArea + log(LotArea)) ^ 2, data = train)

trans_mod_2 = lm(SalePrice ~ . - LotArea + log(LotArea) + I(TotalBsmtSF ^ 2) + I(YearBuilt ^ 2), data = train)

trans_mod_3 = lm(log(SalePrice) ~ . - LotArea - BedroomAbvGr + log(LotArea) + log(BedroomAbvGr), data = train)

trans_mod_4 = lm(log(SalePrice) ~ . - LotArea - BedroomAbvGr + log(LotArea) + log(BedroomAbvGr) + I(TotalBsmtSF ^ 2) + I(YearBuilt ^ 2), data = train)
```

- Right now, we have 9 candidate models in total:
  - model_add
  - model_inter
  - log_add
  - log_inter
  - trans_mod_1
  - trans_mod_1_inter
  - trans_mod_2
  - trans_mod_3
  - trans_mod_4

### Model Choosing

- In the next step, we will use BIC to choose the best model for us, and for the additive models, we will use both backward and stepwise search.
```{r}
n = nrow(train) # for BIC

# additive model
add_backward = step(model_add, direction = 'backward', trace = 0, k = log(n))
add_forward = step(lm(SalePrice ~ 1, data = train),
                   scope = SalePrice ~ LotArea + MSZoning + LotShape + OverallQual + YearBuilt + TotalBsmtSF + LowQualFinSF + BedroomAbvGr + FullBath + GarageArea, direction = 'both', trace = 0, k = log(n))

get_adj_r2(add_backward)
get_adj_r2(add_forward)
get_loocv_rmse(add_backward)
get_loocv_rmse(add_forward)

# interaction model 
inter_backward = step(model_inter, direction = 'backward', k = log(n), trace = 0)

get_adj_r2(inter_backward)
get_loocv_rmse(inter_backward)

# log additive model
log_add_backward = step(log_add, direction = 'backward', trace = 0, k = log(n))
log_add_forward = step(lm(log(SalePrice) ~ 1, data = train), 
                       scope = log(SalePrice) ~ LotArea + MSZoning + LotShape + OverallQual + YearBuilt + TotalBsmtSF + LowQualFinSF + BedroomAbvGr + FullBath + GarageArea, trace = 0, direction = 'forward', k = log(n))

get_adj_r2(log_add_backward)
get_adj_r2(log_add_forward)
get_loocv_rmse(log_add_backward) # log response
get_loocv_rmse(log_add_forward) # log response

# log interaction model
log_inter_backward = step(log_inter, direction = 'backward', trace = 0, k = log(n))

get_adj_r2(log_inter_backward)
get_loocv_rmse(log_inter_backward) # log response

# transformation model 1
trans_1_backward = step(trans_mod_1, direction = 'backward', trace = 0, k = log(n))
trans_1_inter = step(trans_mod_1_inter, direction = 'backward', trace = 0, k = log(n))

get_adj_r2(trans_1_backward)
get_adj_r2(trans_1_inter)
get_loocv_rmse(trans_1_backward) # log response
get_loocv_rmse(trans_1_inter) # log response

# transformation model 2
trans_2_backward = step(trans_mod_2, direction = 'backward', trace = 0, k = log(n))

get_adj_r2(trans_2_backward)
get_loocv_rmse(trans_2_backward)

# transformation model 3
trans_3_backward = step(trans_mod_3, direction = 'backward', trace = 0, k = log(n))

get_adj_r2(trans_3_backward) 
get_loocv_rmse(trans_3_backward) # log response

# transformation model 4
trans_4_backward = step(trans_mod_4, direction = 'backward', k = log(n), trace = 0)

get_adj_r2(trans_4_backward)
get_loocv_rmse(trans_4_backward) # log response
```

Since some of the models have log transformed response, the non-log model and log model cannot be compared with LOOCV_RMSE directly, we will choose the best models of each kind.