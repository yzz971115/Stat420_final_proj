---
title: "Predict Prices for Your Dream House"
author: "STAT 420, Summer 2019, Zhenzhou Yang (zy29), Swan Htun (swanh2), Mike Kramer (mkramer4)"
date: '7/16/2019'
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---

# Introduction

## Overview

In this project, we plan to find a suitable linear model to predict the prices of residential houses locating in Ames, Iowa, based on their attributes provided. This model is meaningful that the customers who want to buy a new house may rely on it to have a rough estimation. 

In this project, many of the topics will be included, some of them will be:

- Multiple linear regression
- Outlier diagnostics
- Model building 
- Model selection

## Dataset Introduction

### Source

The `Ames Housing dataset` we use in this project is provided by Dean De Cock from Truman State University for [`Kaggle competition`](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data).

### Introduction

The origin dataset mainly includes the sale of individual residential houses in Ames, Iowa from 2006 to 2010. 

It contains 2919 observations with 23 nominal, 23 ordinal, 14 discrete and 20 continuous variables, which are directly related to property sales. And we just use half of it which contains 1460 observations and split the data by half into our train and test dataset.

In our project, we choose the variables which we think have significant influence in affecting the price of a house to somplify the data and try to reduce the collinearity between different predictors at beginning:

- **SalePrice**: the property's sale price in dollars. This is our target to predict.
- **LotArea**: Lot size in square feet.
- **MSZoning**: Identifies the general zoning classification of the sale. It has following types:
  - A:	Agriculture
  - C:	Commercial
  - FV:	Floating Village Residential
  - I:	Industrial
  - RH:	Residential High Density
  - RL:	Residential Low Density
  - RP:	Residential Low Density Park 
  - RM:	Residential Medium Density
- **LotShape**: General shape of property. It has the following levels:
  - Reg:	Regular	
  - IR1:	Slightly irregular
  - IR2:	Moderately Irregular
  - IR3:	Irregular
- **OverallQual**: Overall material and finish quality. It is an ordinal categorical variable range from 1 to 10 in origin data, which indicating the quality from very poor to very excellent.
- **YearBuilt**: Original construction date.
- **TotalBsmtSF**: Total square feet of basement area.
- **LowQualFinSF**: Low quality finished square feet (all floors).
- **BedroomAbvGr**: Bedrooms above grade (does NOT include basement bedrooms).
- **FullBath**: Full bathrooms above grade.
- **GarageArea**: Size of garage in square feet.

### Data

In this section, we will take a look at the data which has been modified by us for the project.

- Train data
```{r,message=FALSE}
library(readr)
data_raw = read_csv("house price.csv")

# split the data
set.seed(51)
train_idx = sample(1 : nrow(data_raw), nrow(data_raw) / 2)
test_idx = setdiff(seq(1 : nrow(data_raw)), train_idx)

train_raw = data_raw[train_idx,]
test_raw = data_raw[test_idx,]

# select variables we want to use
train = subset(train_raw, select = c("SalePrice", "LotArea", "MSZoning", "LotShape", "OverallQual", "YearBuilt", "TotalBsmtSF", "LowQualFinSF", "BedroomAbvGr", "FullBath", "GarageArea"))

# show a few lines
head(train, 5)

# let's take a galance at the response variable
head(train$SalePrice, 10)
```

- Test data
```{r,message=FALSE} 
# select variables we want to use
test = subset(test_raw, select = c("SalePrice", "LotArea", "MSZoning", "LotShape", "OverallQual", "YearBuilt", "TotalBsmtSF", "LowQualFinSF", "BedroomAbvGr", "FullBath", "GarageArea"))

# show a few lines
head(test, 5)

# let's take a galance at the response variable
head(test$SalePrice, 10)
```

# Methods
 
## Data Checking

- First of all, we will check the validality of our data and omit the missing data.
```{r}
# train set
sum(is.na(train))

# test set
sum(is.na(test))
```

It seems that our data set is very good! There is no missing data in it.

- The next step is to convert our categorical variables into factor.
```{r}
# train set
train$MSZoning = as.factor(train$MSZoning)
train$LotShape = as.factor(train$LotShape)

# test set
test$MSZoning = as.factor(test$MSZoning)
test$LotShape = as.factor(test$LotShape)
```

And also, since `OverallQual` has 10 levels which maybe too complicated for us to use, we just modify it into three different levels, the original level 1-3 will be **poor**, level 4-7 will be **average**, the rest, level 8-10, will be **excellent**.
```{r}
# train set
train$OverallQual[train$OverallQual == 3 | train$OverallQual == 2 | train$OverallQual == 1] = "poor"
train$OverallQual[train$OverallQual == 4 | train$OverallQual == 5 | train$OverallQual == 6 | train$OverallQual == 7] = "average"
train$OverallQual[train$OverallQual == 8 | train$OverallQual == 9 | train$OverallQual == 10] = "excellent"

train$OverallQual = as.factor(train$OverallQual)

# test set
test$OverallQual[test$OverallQual == 3 | test$OverallQual == 2 | test$OverallQual == 1] = "poor"
test$OverallQual[test$OverallQual == 4 | test$OverallQual == 5 | test$OverallQual == 6 | test$OverallQual == 7] = "average"
test$OverallQual[test$OverallQual == 8 | test$OverallQual == 9 | test$OverallQual == 10] = "excellent"

test$OverallQual = as.factor(test$OverallQual)
```

- Then, we will to check the distribution of our target -- `SalePrice`. 
```{r}
hist(train$SalePrice, col = 'darkorange', main = 'Sale Price', prob = TRUE)
```

It seems that the `SalePrice` is right skewed, so we will try to make some transformations to make it look better.
```{r}
hist(log(train$SalePrice), col = 'darkorange', main = 'Sale Price', prob = TRUE)
qqnorm(log(train$SalePrice), col = "darkgrey", pch = 20, cex= 1.25)
qqline(log(train$SalePrice), col = "darkorange")
```

After the log transformation, the distribution looks much better now.

## Predictor Choosing

- We first modify the factor variables into numeric ones to check the correlation between each variables.
```{r}
p = train$OverallQual

# convert variables
train$MSZoning = as.numeric(train$MSZoning)
train$LotShape = as.numeric(train$LotShape)
train$OverallQual = as.numeric(train$OverallQual)

# check correlation
round(cor(train), 2)

# convert back 
train$MSZoning = as.factor(train$MSZoning)
train$LotShape = as.factor(train$LotShape)
train$OverallQual = as.factor(train$OverallQual)

# rename back the factor variables
levels(train$LotShape) = c("IR1", "IR2", "IR3", "Reg")
levels(train$MSZoning) = c("C (all)", "FV", "RH", "RL", "RM")
levels(train$OverallQual) = c("average", "excellent", "poor")
```

From the table above, we do not find any significant colliearity between different variables. We can choose all of them together for fitting.

## Model Fitting

- Before we create any models, we prefer to define some functions we may use later first.
```{r,message=FALSE}
library(lmtest)

# Fitted versus Residuals Plot
plot_fitted_resid = function(model, pointcol = "darkgrey", linecol = "darkorange") {
  plot(fitted(model), resid(model), 
       col = pointcol, pch = 20, cex = 1.25,
       xlab = "Fitted", ylab = "Residuals")
  abline(h = 0, col = linecol, lwd = 2)
}

# Q-Q plot
plot_qq = function(model, pointcol = "darkgrey", linecol = "darkorange") {
  qqnorm(resid(model), col = pointcol, pch = 20, cex = 1.25)
  qqline(resid(model), col = linecol, lwd = 2)
}

# bptest
get_bp_decision = function(model, alpha = 0.05) {
  decide = unname(bptest(model)$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

# shapiro wilk test
get_sw_decision = function(model, alpha = 0.05) {
  decide = unname(shapiro.test(resid(model))$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

# LOOCV_RMSE
get_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

# ajusted R2
get_adj_r2 = function(model) {
  summary(model)$adj.r.squared
}
```

### Candidate Models

- Firstly, we will start with an intersection model with all possible two-way interaction between predictors and an additive model with all possible predictors.
```{r}
model_add = lm(SalePrice ~ ., data = train)
model_inter = lm(SalePrice ~ . ^ 2, data = train)
```

- We will also try to find some models with transformed variablesã€‚

As we analyzed before, after the log transformation, the SalePrice will be more normalized.
```{r}
log_add = lm(log(SalePrice) ~ ., data = train)
log_inter = lm(log(SalePrice) ~ . ^ 2, data = train)
```

Then, we will check the realtionship between `SalePrice` and each numerical predictors
```{r}
par(mfrow = c(3, 2))
plot(SalePrice ~ LotArea, data = train, col = "darkgrey")
plot(SalePrice ~ YearBuilt, data = train, col = "darkgrey")
plot(SalePrice ~ TotalBsmtSF, data = train, col = "darkgrey")
plot(SalePrice ~ LowQualFinSF, data = train, col = "darkgrey")
plot(SalePrice ~ BedroomAbvGr, data = train, col = "darkgrey")
```

We try to use some transformations to make the plot looks like more linearly.
```{r}
par(mfrow = c(3, 2))
plot(log(SalePrice) ~ log(LotArea), data = train, col = "darkgrey")
plot(log(SalePrice) ~ YearBuilt, data = train, col = "darkgrey")
plot(log(SalePrice) ~ TotalBsmtSF, data = train, col = "darkgrey")
plot(SalePrice ~ I(TotalBsmtSF ^ 2), data = train, col = "darkgrey")
plot(log(SalePrice) ~ LowQualFinSF, data = train, col = "darkgrey")
```

Based on the above plots, we add several new models.
```{r}
trans_mod_1 = lm(log(SalePrice) ~ . - LotArea + log(LotArea), data = train)
trans_mod_1_inter = lm(log(SalePrice) ~ (. - LotArea + log(LotArea)) ^ 2, data = train)

trans_mod_2 = lm(SalePrice ~ . - LotArea + log(LotArea) + I(TotalBsmtSF ^ 2) + I(YearBuilt ^ 2), data = train)

trans_mod_3 = lm(log(SalePrice) ~ . - LotArea + log(LotArea), data = train)

trans_mod_4 = lm(log(SalePrice) ~ . - LotArea + log(LotArea) + I(TotalBsmtSF ^ 2) + I(YearBuilt ^ 2), data = train)
```

- Right now, we have 9 candidate models in total:
  - model_add
  - model_inter
  - log_add
  - log_inter
  - trans_mod_1
  - trans_mod_1_inter
  - trans_mod_2
  - trans_mod_3
  - trans_mod_4

### Model Choosing

- In the next step, we will use BIC to choose the best model for us, and for the additive models, we will use both backward and stepwise search.
```{r}
n = nrow(train) # for BIC

# additive model
add_backward = step(model_add, direction = 'backward', trace = 0, k = log(n))
add_stepwise = step(lm(SalePrice ~ 1, data = train),
                   scope = SalePrice ~ LotArea + MSZoning + LotShape + OverallQual + YearBuilt + TotalBsmtSF + LowQualFinSF + BedroomAbvGr + FullBath + GarageArea, direction = 'both', trace = 0, k = log(n))

get_adj_r2(add_backward)
get_adj_r2(add_stepwise)
get_loocv_rmse(add_backward)
get_loocv_rmse(add_stepwise)

# interaction model 
inter_backward = step(model_inter, direction = 'backward', k = log(n), trace = 0)

get_adj_r2(inter_backward)
get_loocv_rmse(inter_backward)

# log additive model
log_add_backward = step(log_add, direction = 'backward', trace = 0, k = log(n))
log_add_stepwise = step(lm(log(SalePrice) ~ 1, data = train), 
                       scope = log(SalePrice) ~ LotArea + MSZoning + LotShape + OverallQual + YearBuilt + TotalBsmtSF + LowQualFinSF + BedroomAbvGr + FullBath + GarageArea, trace = 0, direction = 'both', k = log(n))

get_adj_r2(log_add_backward)
get_adj_r2(log_add_stepwise)
get_loocv_rmse(log_add_backward) # log response
get_loocv_rmse(log_add_stepwise) # log response

# log interaction model
log_inter_backward = step(log_inter, direction = 'backward', trace = 0, k = log(n))

get_adj_r2(log_inter_backward)
get_loocv_rmse(log_inter_backward) # log response

# transformation model 1
trans_1_backward = step(trans_mod_1, direction = 'backward', trace = 0, k = log(n))
trans_1_inter = step(trans_mod_1_inter, direction = 'backward', trace = 0, k = log(n))

get_adj_r2(trans_1_backward)
get_adj_r2(trans_1_inter)
get_loocv_rmse(trans_1_backward) # log response
get_loocv_rmse(trans_1_inter) # log response

# transformation model 2
trans_2_backward = step(trans_mod_2, direction = 'backward', trace = 0, k = log(n))

get_adj_r2(trans_2_backward)
get_loocv_rmse(trans_2_backward)

# transformation model 3
trans_3_backward = step(trans_mod_3, direction = 'backward', trace = 0, k = log(n))

get_adj_r2(trans_3_backward) 
get_loocv_rmse(trans_3_backward) # log response

# transformation model 4
trans_4_backward = step(trans_mod_4, direction = 'backward', k = log(n), trace = 0)

get_adj_r2(trans_4_backward)
get_loocv_rmse(trans_4_backward) # log response
```

- Since some of the models have log transformed response, the non-log model and log model cannot be compared with LOOCV_RMSE directly, we will choose the best models of each kind.

Based on the results above, we can find that for non-log model, the `inter_backward` model has the highest adjusted $R ^ 2$ and lowest LOOCV_RMSE. For log model, the `trans_4_backward` model has the lowest LOOCV_RMSE and `trans_1_inter` model has the highest adjusted $R ^ 2$.

With this three model, we will use vif to check the collinearity and summary to find the significant predictors for us.
```{r}
library(faraway)

vif(inter_backward)
summary(inter_backward)$coef[, 'Pr(>|t|)'] < 0.05

vif(trans_4_backward)
summary(trans_4_backward)$coef[, 'Pr(>|t|)'] < 0.05

vif(trans_1_inter)
summary(trans_1_inter)$coef[, 'Pr(>|t|)'] < 0.05
```

Based on the results above, we try to modify some parameters for our models to remove the collinearity problem and unnecessary predictors.
```{r}
inter_backward_new = lm(SalePrice ~ LotShape + OverallQual + YearBuilt + TotalBsmtSF + BedroomAbvGr + LotArea : OverallQual, data = train)

trans_4_backward_new = lm(log(SalePrice) ~ MSZoning + OverallQual + TotalBsmtSF + FullBath + GarageArea + log(LotArea) + I(TotalBsmtSF ^ 2) + I(YearBuilt ^ 2), data = train)

trans_1_inter_new = lm(log(SalePrice) ~ log(LotArea) + MSZoning + OverallQual + YearBuilt + TotalBsmtSF, data = train)
```

Then we will use `ANOVA` to compare each new model with the previous.
```{r}
# inter_backward model
anova(inter_backward_new, inter_backward)

# trans_4_backward model
anova(trans_4_backward_new, trans_4_backward)

# log_inter_backward model
anova(trans_1_inter_new, trans_1_inter)
```

From the results of `ANOVA`, we find that all of the three origin models are preferred. So we decide not to change the models since collinearity will not affect prediction.

- With this three models, we will use both train and test data to calculate base RMSE to compare the log and non-log models.
```{r}
# inter_backward model
inter_train = sqrt(mean(resid(inter_backward) ^ 2))

test_pred_1 = predict(inter_backward, newdata = test)
inter_test = sqrt(mean((test$SalePrice - test_pred_1) ^ 2))

# trans_4_backward
trans_train = sqrt(mean((train$SalePrice - exp(fitted(trans_4_backward))) ^ 2))

test_pred_2 = exp(predict(trans_4_backward, newdata = test))
trans_test = sqrt(mean((test$SalePrice - test_pred_2) ^ 2))

# trans_1_inter
trans_inter_train = sqrt(mean((train$SalePrice - exp(fitted(trans_1_inter))) ^ 2))

test_pred_3 = exp(predict(trans_1_inter, newdata = test))
trans_inter_test = sqrt(mean((test$SalePrice - test_pred_3) ^ 2))

# create a table to compare the result
library(knitr)
result = data.frame('inter_model' = c('train' = inter_train, 'test' = inter_test), 'trans_model' = c('train' = trans_train, 'test' = trans_test), 'trans_inter_model' = c('train' = trans_inter_train, 'test' = trans_inter_test))
kable(result)
```

From the table above, we can see that the `trans_4_backward` model has the lowest test RMSE as well as the best train RMSE.

Therefore, we prefer to choose the `trains_4_backward` model, which having the following coefficients:
```{r}
coef(trans_4_backward)
```

### Model Diagnostics

After selecting the preferred model, we are going to verify the model assumptions.
```{r}
plot_fitted_resid(trans_4_backward)
plot_qq(trans_4_backward)
```

From the `fitted vs residual` plot and `qq plot`, it seems that the constant variance and normality assumptions are a little suspect. 

```{r}
get_sw_decision(trans_4_backward)
get_bp_decision(trans_4_backward)
```

And also, Breusch-Pagan Test and Shapiro Wilk Test verify the violation of assumptions, they both reject the null under $\alpha = 0.05$.

To fix the problem, we first try to remove the high influential observations.
```{r}
# Cook's Distance 
cd_trans_model = cooks.distance(trans_4_backward)

model_fix = lm(log(SalePrice) ~ MSZoning + LotShape + OverallQual + YearBuilt + TotalBsmtSF + BedroomAbvGr + FullBath + GarageArea + log(LotArea) + I(TotalBsmtSF ^ 2) + I(YearBuilt ^ 2), data = train, subset = cd_trans_model <= 4 / length(cd_trans_model))

# check the new model again
plot_fitted_resid(model_fix)
plot_qq(model_fix)

get_sw_decision(model_fix)
get_bp_decision(model_fix)
```

From the two plots, we can see that they look much better than before, although it still cannot pass the two assumption tests.
